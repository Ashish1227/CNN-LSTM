{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Reshape, Dense, LSTM, Embedding, Dropout, add, concatenate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BASE_DIR = '/kaggle/input/flickr8vr1/Flickr8K'\n","WORKING_DIR = '/kaggle/working'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#load vgg16 model\n","model = VGG16()\n","# Restructure the model\n","model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","# summarize\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# extarct the feature from the image\n","# we store test features and train features seperately.\n","train_features = {}\n","test_features = {}\n","tr_directory = os.path.join(BASE_DIR, 'Flickr8k_text/Flickr_8k.trainImages.txt')\n","te_directory = os.path.join(BASE_DIR, 'Flickr8k_text/Flickr_8k.testImages.txt')\n","directory = os.path.join(BASE_DIR, 'Flicker8k_Images')\n","\n","with open(tr_directory) as f:\n","    tr_img_names = f.readlines()\n","tr_img_names = [x.strip() for x in tr_img_names]\n","\n","with open(te_directory) as g:\n","    te_img_names = g.readlines() \n","te_img_names = [x.strip() for x in te_img_names]\n","\n","for img_name in tqdm(tr_img_names):\n","    # load the image file\n","    img_path = directory  + '/' + img_name\n","    image = load_img(img_path, target_size=(224,224))\n","    # converts image pixels to numpy array\n","    image = img_to_array(image)\n","    #image reshape data for model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # preprocess image for vgg\n","    image = preprocess_input(image)\n","    #extract features\n","    feature = model.predict(image, verbose=0)\n","    #get image ID\n","    image_id = img_name.split('.')[0]\n","    # store feature\n","    train_features[image_id]=feature\n","    \n","for img_name in tqdm(te_img_names):\n","    # load the image file\n","    img_path = directory  + '/' + img_name\n","    image = load_img(img_path, target_size=(224,224))\n","    # converts image pixels to numpy array\n","    image = img_to_array(image)\n","    #image reshape data for model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # preprocess image for vgg\n","    image = preprocess_input(image)\n","    #extract features\n","    feature = model.predict(image, verbose=0)\n","    #get image ID\n","    image_id = img_name.split('.')[0]\n","    # store feature\n","    test_features[image_id]=feature"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# #store features in pickle\n","# pickle.dump(train_features, open(os.path.join(WORKING_DIR, 'tr_features.pkl'), 'wb'))\n","# pickle.dump(test_features, open(os.path.join(WORKING_DIR, 'te_features.pkl'), 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# #load features from pickle\n","# with open(os.path.join('/kaggle/input/firstall/tr_features.pkl'), 'rb') as f:\n","#     train_features = pickle.load(f)\n","    \n","# with open(os.path.join('/kaggle/input/firstall/te_features.pkl'), 'rb') as f:\n","#     test_features = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#load caption \n","with open(os.path.join(BASE_DIR, 'Flickr8k_text/Flickr8k.token.txt'),'r') as f:\n","    captions_doc = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create mapping of image to captions\n","mapping = {}\n","# process lines\n","for line in tqdm(captions_doc.split('\\n')):\n","    #split the line by tab\n","    tokens = line.split('\\t')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    #remove extension from image id\n","    image_id = image_id.split('.')[0]\n","    # convert caption list to string\n","    caption = \" \".join(caption)\n","    # create list if needed\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    mapping[image_id].append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# preprocess caption function\n","def clean(mapping):\n","    for key, captions in mapping.items():\n","        for i in range(len(captions)):\n","            # tkae one captin at a time\n","            caption = captions[i]\n","            # preprocessing text\n","            # convert to lower case\n","            caption = caption.lower()\n","            # delete digits and special characters etc.,\n","            caption = caption.replace('[A=Za-z]','')\n","            # delete addtional spaces\n","            caption = caption.replace('\\s+', ' ')\n","            # add start and end tags to the caption\n","            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1])  + ' endseq'\n","            captions[i]= caption"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Before preprocess of text\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# preprocess the text\n","clean(mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# After preprocess of text\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["all_captions = []\n","for key in mapping:\n","    for caption in mapping[key]:\n","        all_captions.append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(all_captions)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_captions)\n","vocab_size=len(tokenizer.word_index)+1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get maximum length of the captions avilable\n","max_length = max(len(caption.split()) for caption in all_captions)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# craete data generator to get data in batch (avoid session crash)\n","def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n","    X1, X2, y =list(), list(), list()\n","    n = 0\n","    while 1:\n","        for key in data_keys:\n","            n +=1\n","            captions = mapping[key]\n","            # process each caption\n","            for caption in captions:\n","                # encode the sequence\n","                seq = tokenizer.texts_to_sequences([caption])[0]\n","                #split the sequence into X, Y pairs\n","                for i in range(1, len(seq)):\n","                    #split into input and output pairs\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    # pad input seqence\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    # encode output sequence\n","                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","                    \n","                    # store the sequences\n","                    X1.append(features[key][0])\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            if n == batch_size:\n","                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","                yield [X1, X2], y\n","                X1, X2, y = list(), list(), list()\n","                n = 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embeddings_index = {} \n","f = open('../input/glovevr1/glove.840B.300d.txt', encoding=\"utf-8\")\n","for line in f:\n","    values = line.split(' ')\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding_dim = len(list(embeddings_index.values())[0])\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["inputs1 = Input(shape=(4096,))\n","inputs2 = Input(shape=(max_length,))\n","\n","fe1 = Dropout(0.5)(inputs1)\n","fe2 = Dense(200, activation='relu')(fe1)\n","fe3 = Reshape((1, 200), input_shape=(200,))(fe2)\n","\n","se1 = Embedding(vocab_size, embedding_dim, mask_zero=True, weights=[embedding_matrix])(inputs2)\n","merged = concatenate([fe3, se1], axis = 1)\n","se2 = LSTM(200)(merged)\n","se3 = Dropout(0.5)(se2)\n","\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(200, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","plot_model(model, show_shapes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tr_img_names = [x.split('.')[0] for x in tr_img_names]\n","te_img_names = [x.split('.')[0] for x in te_img_names]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train the model\n","epochs = 15\n","batch_size = 64\n","steps = len(tr_img_names) // batch_size\n","\n","for i in range(epochs):\n","    # create data generator\n","    generator = data_generator(tr_img_names, mapping, train_features, tokenizer, max_length, vocab_size, batch_size)\n","    # fit for one epoch\n","    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # save the model\n","# from tensorflow import keras\n","# model = keras.models.load_model('/kaggle/input/firstall/first_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def idx_to_word(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# generate caption for an image\n","def predict_caption(model, image, tokenizer, max_length):\n","    # add start tag for generaation process\n","    in_text = 'startseq'\n","    # iterate over the max length of sequence\n","    for i in range(max_length):\n","        # encode input sequence\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        # pad sequences\n","        sequence = pad_sequences([sequence], max_length)\n","        # predict next word\n","        yhat = model.predict([image, sequence], verbose=0)\n","        #get index with high probability\n","        yhat = np.argmax(yhat)\n","        # convert index to word\n","        word = idx_to_word(yhat, tokenizer)\n","        #stop if word not found\n","        if word is None:\n","            break\n","        # append word as input for generating next word\n","        in_text+=\" \"+word\n","        # stop if we reach end tag\n","        if word == 'endseq':\n","            break\n","    return in_text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu\n","# validate with test data\n","actual, predicted = list(), list()\n","\n","for key in tqdm(te_img_names):\n","    #get actual caption\n","    captions= mapping[key]\n","    # predict the caption for image\n","    y_pred = predict_caption(model, test_features[key], tokenizer, max_length)\n","    #split ino words\n","    actual_captions = [caption.split() for caption in captions]\n","    y_pred = y_pred.split()\n","    #append to the list\n","    actual.append(actual_captions)\n","    predicted.append(y_pred)\n","    \n","# calculate BLEU score\n","print(\"BLEU-1:%f\"% corpus_bleu(actual, predicted, weights=(1.0,0,0,0)))\n","print(\"BLEU-2:%f\"% corpus_bleu(actual, predicted, weights=(0.5,0.5,0,0)))\n","print(\"BLEU-3:%f\"% corpus_bleu(actual, predicted, weights=(1/3,1/3,1/3,0)))\n","print(\"BLEU-4:%f\"% corpus_bleu(actual, predicted, weights=(0.25,0.25,0.25,0.25)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Uninstall the previous installed nltk library\n","# !pip install -U nltk\n","\n","# # This upgraded nltkto version 3.5 in which meteor_score is there.\n","# !pip install nltk==3.5\n","\n","# from nltk.translate.meteor_score import meteor_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # validate with test data\n","# actual, predicted = list(), list()\n","# avg_score = 0\n","# count = 0\n","# for key in tqdm(te_img_names):\n","    \n","#     #get actual caption\n","#     captions= mapping[key]\n","#     # predict the caption for image\n","#     y_pred = predict_caption(model, test_features[key], tokenizer, max_length)\n","#     actual_captions = [\" \".join(caption) for caption in captions]\n","#     avg_score = avg_score + meteor_score(actual_captions, y_pred)\n","#     count+=1\n","\n","# print(\"average meteor score is :\",avg_score/count)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","def generate_caption(image_name):\n","    # load the image\n","    #image_name = \"\"\n","    image_id = image_name.split('.')[0]\n","    img_path = os.path.join(BASE_DIR, \"Flicker8k_Images\", image_name)\n","    image = Image.open(img_path)\n","    captions = mapping[image_id]\n","    print('-------------Actual------------')\n","    for caption in captions:\n","        print(caption)\n","    y_pred = predict_caption(model, train_features[image_id], tokenizer, max_length)\n","    print('--------------Predicted---------------')\n","    print(y_pred)\n","    plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generate_caption(\"1001773457_577c3a7d70.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
